{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87baf8f3",
   "metadata": {},
   "source": [
    "Setting up Doom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eb9e0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipinstall vizdoom and clone repo in new folder\n",
    "#!cd github & git clone https://github.com/Farama-Foundation/ViZDoom.git\n",
    "\n",
    "#import vizdoom to setup game environment\n",
    "from vizdoom import *\n",
    "\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#pip install and import gym environment\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "37732746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "class VizDoomGym(Env):\n",
    "    #called when env is started > game\n",
    "    def __init__(self,render=False):\n",
    "        \n",
    "        #inherit from Env import\n",
    "        super().__init__()\n",
    "        \n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(\"github/VizDoom/scenarios/basic.cfg\")\n",
    "        \n",
    "        #Disable or enable window visiblity when game is running\n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        self.game.init()\n",
    "    \n",
    "        \n",
    "        \n",
    "        #create obs and action space\n",
    "        #low/high indicates pixel vals\n",
    "        self.observation_space = Box(low=0,high=255,shape=(100,160,1),dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "    #tale actons\n",
    "    def step(self,action):\n",
    "        actions = np.identity(3,dtype=np.uint8)\n",
    "        \n",
    "        #take action, make_action() returns reward value for taking ste\n",
    "        #2nd para is frame skip to give time between taking action and receiving result\n",
    "        reward = self.game.make_action(actions[action],4)\n",
    "        \n",
    "        #if something is returned from game_state()\n",
    "        if self.game.get_state():\n",
    "            #get game state to grab screen image\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            #apply grayscale\n",
    "            state = self.grayscale(state)\n",
    "            #use game state to grab game vars, i.e. ammo\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            info = ammo\n",
    "        #game_state returns nothing/errors out\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0\n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        \n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        \n",
    "        return state,reward,done,info\n",
    "    def render():\n",
    "        pass\n",
    "    \n",
    "    #resets game\n",
    "    def reset(self):\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    #grayscale frame and scales down image, to make training faster\n",
    "    def grayscale(self,observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation,0,-1),cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100),interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100,160,1))\n",
    "        return state\n",
    "    #close the game\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "813b5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7b3a1b",
   "metadata": {},
   "source": [
    "Setting up callback for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cd2162cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves tensorboard log file after training, go into PPO_n and run tensorboard --logdir=. then open local host link\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "    \n",
    "CHECKPOINT_DIR = './train/train_basic'\n",
    "LOG_DIR = './logs/log_basic'\n",
    "\n",
    "#after every 10k steps of training model, save version of pytorch weights for RL agent\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e75eb6",
   "metadata": {},
   "source": [
    "Proximal Policy Optimization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab9b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_basic\\PPO_2\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36       |\n",
      "|    ep_rew_mean     | -108     |\n",
      "| time/              |          |\n",
      "|    fps             | 54       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.4        |\n",
      "|    ep_rew_mean          | -97.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 36          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007562112 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -8.88e-05   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 875         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.00332     |\n",
      "|    value_loss           | 2.17e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 29.4         |\n",
      "|    ep_rew_mean          | -66.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 32           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 191          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095325485 |\n",
      "|    clip_fraction        | 0.171        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0524       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.61e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.00118      |\n",
      "|    value_loss           | 3.39e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.4        |\n",
      "|    ep_rew_mean          | -67         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 273         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007577762 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.217       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.37e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.00162     |\n",
      "|    value_loss           | 2.9e+03     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23.4        |\n",
      "|    ep_rew_mean          | -33.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 347         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012536997 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.34e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00062    |\n",
      "|    value_loss           | 2.94e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 18.7       |\n",
      "|    ep_rew_mean          | -4.77      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 29         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 422        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03145458 |\n",
      "|    clip_fraction        | 0.214      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.624      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.27e+03   |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | 0.001      |\n",
      "|    value_loss           | 3.03e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.8        |\n",
      "|    ep_rew_mean          | 14.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 496         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011304123 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.26e+03    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.000829    |\n",
      "|    value_loss           | 2.86e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 16.7        |\n",
      "|    ep_rew_mean          | 6.06        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 574         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013125276 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.35e+03    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.00166     |\n",
      "|    value_loss           | 3.39e+03    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Non rendered environment\n",
    "env = VizDoomGym()\n",
    "#pass convolutional neural network, cnn for image\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=2048)\n",
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883399a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from disc\n",
    "model = PPO.load('./train/train_basic/best_model_60000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3040d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create rendered environment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d49d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate mean reward for 10 games\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f37d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(100): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # time.sleep(0.20)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(total_reward, episode))\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
