{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87baf8f3",
   "metadata": {},
   "source": [
    "Setting up Doom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb9e0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipinstall vizdoom and clone repo in new folder\n",
    "#!cd github & git clone https://github.com/Farama-Foundation/ViZDoom.git\n",
    "\n",
    "#import vizdoom to setup game environment\n",
    "from vizdoom import *\n",
    "\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#pip install and import gym environment\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37732746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env):\n",
    "    #called when env is started > game\n",
    "    def __init__(self,render=False):\n",
    "        \n",
    "        #inherit from Env import\n",
    "        super().__init__()\n",
    "        \n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(\"github/VizDoom/scenarios/basic.cfg\")\n",
    "        \n",
    "        #Disable or enable window visiblity when game is running\n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "        \n",
    "        self.game.init()\n",
    "    \n",
    "        \n",
    "        \n",
    "        #create obs and action space\n",
    "        #low/high indicates pixel vals\n",
    "        self.observation_space = Box(low=0,high=255,shape=(100,160,1),dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "    #tale actons\n",
    "    def step(self,action):\n",
    "        actions = np.identity(3,dtype=np.uint8)\n",
    "        \n",
    "        #take action, make_action() returns reward value for taking ste\n",
    "        #2nd para is frame skip to give time between taking action and receiving result\n",
    "        reward = self.game.make_action(actions[action],4)\n",
    "        \n",
    "        #if something is returned from game_state()\n",
    "        if self.game.get_state():\n",
    "            #get game state to grab screen image\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            #apply grayscale\n",
    "            state = self.grayscale(state)\n",
    "            #use game state to grab game vars, i.e. ammo\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            info = ammo\n",
    "        #game_state returns nothing/errors out\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0\n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        \n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        \n",
    "        return state,reward,done,info\n",
    "    def render():\n",
    "        pass\n",
    "    \n",
    "    #resets game\n",
    "    def reset(self):\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    #grayscale frame and scales down image, to make training faster\n",
    "    def grayscale(self,observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation,0,-1),cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100),interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize,(100,160,1))\n",
    "        return state\n",
    "    #close the game\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3b07ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79b1ef7",
   "metadata": {},
   "source": [
    "Setting up callback for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd66522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves tensorboard log file after training, go into PPO_n and run tensorboard --logdir=. then open local host link\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "    \n",
    "CHECKPOINT_DIR = './train/train_basic'\n",
    "LOG_DIR = './logs/log_basic'\n",
    "\n",
    "#after every 10k steps of training model, save version of pytorch weights for RL agent\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de597f4e",
   "metadata": {},
   "source": [
    "Proximal Policy Optimization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2dc3bd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileDoesNotExistException",
     "evalue": "File \"github/VizDoom/scenarios/basic.cfg | ./scenarios/github/vizdoom/scenarios/basic.cfg | C:/Users/ahmed/anaconda3/lib/site-packages/vizdoom/scenarios/github/vizdoom/scenarios/basic.cfg\" does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileDoesNotExistException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-cfad5025848f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Non rendered environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVizDoomGym\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#pass convolutional neural network, cnn for image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CnnPolicy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard_log\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLOG_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-b393eb591340>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, render)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoomGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"github/VizDoom/scenarios/basic.cfg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#Disable or enable window visiblity when game is running\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileDoesNotExistException\u001b[0m: File \"github/VizDoom/scenarios/basic.cfg | ./scenarios/github/vizdoom/scenarios/basic.cfg | C:/Users/ahmed/anaconda3/lib/site-packages/vizdoom/scenarios/github/vizdoom/scenarios/basic.cfg\" does not exist."
     ]
    }
   ],
   "source": [
    "# Non rendered environment\n",
    "env = VizDoomGym()\n",
    "#pass convolutional neural network, cnn for image\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=2048)\n",
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61390571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6b342bd77f481a7a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6b342bd77f481a7a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir {log_basic/PPO_1}  --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f20d07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e5161a17aec0b059\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e5161a17aec0b059\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir=PPO_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36ec10d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7a95089b4e60b99a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7a95089b4e60b99a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir=\"C:\\Users\\ahmed\\OneDrive\\Desktop\\Training-an-AI-to-play-Doom\\logs\\log_basic\\PPO_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ac13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "413e9c79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reload model at nk steps, > number of episdoes\n",
    "model10k = PPO.load('./train/train_basic/best_model_30000')\n",
    "model50k = PPO.load('./train/train_basic/best_model_50000')\n",
    "model100k = PPO.load('./train/train_basic/best_model_100000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2621ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rendered environment\n",
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12f286a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying 10k steps model to 100 games\n",
    "# Evaluate mean reward for 100 games using loaded model\n",
    "mean_reward, _ = evaluate_policy(model10k, env, n_eval_episodes=100)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5472e79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying 50k steps model to 100 games\n",
    "mean_reward, _ = evaluate_policy(model50k, env, n_eval_episodes=100)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37998223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.84"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying 100k steps model to 100 games\n",
    "mean_reward, _ = evaluate_policy(model100k, env, n_eval_episodes=100)\n",
    "mean_reward"
   ]
  }
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0004ccf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-08a3f133e145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'obs' is not defined"
     ]
    }
   ],
   "source": [
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "49ad20e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 0 is 95.0\n",
      "Total Reward for episode 1 is -300.0\n",
      "Total Reward for episode 2 is 95.0\n",
      "Total Reward for episode 3 is 95.0\n",
      "Total Reward for episode 4 is 95.0\n",
      "Total Reward for episode 5 is 95.0\n",
      "Total Reward for episode 6 is 95.0\n",
      "Total Reward for episode 7 is 95.0\n",
      "Total Reward for episode 8 is -315.0\n",
      "Total Reward for episode 9 is -300.0\n",
      "Total Reward for episode 10 is 95.0\n",
      "Total Reward for episode 11 is 95.0\n",
      "Total Reward for episode 12 is 66.0\n",
      "Total Reward for episode 13 is 95.0\n",
      "Total Reward for episode 14 is 95.0\n",
      "Total Reward for episode 15 is -300.0\n",
      "Total Reward for episode 16 is 68.0\n",
      "Total Reward for episode 17 is 59.0\n",
      "Total Reward for episode 18 is 70.0\n",
      "Total Reward for episode 19 is 95.0\n",
      "Total Reward for episode 20 is 95.0\n",
      "Total Reward for episode 21 is -300.0\n",
      "Total Reward for episode 22 is 95.0\n",
      "Total Reward for episode 23 is 95.0\n",
      "Total Reward for episode 24 is -300.0\n",
      "Total Reward for episode 25 is 95.0\n",
      "Total Reward for episode 26 is 95.0\n",
      "Total Reward for episode 27 is 41.0\n",
      "Total Reward for episode 28 is 95.0\n",
      "Total Reward for episode 29 is 95.0\n",
      "Total Reward for episode 30 is 95.0\n",
      "Total Reward for episode 31 is 95.0\n",
      "Total Reward for episode 32 is 95.0\n",
      "Total Reward for episode 33 is 64.0\n",
      "Total Reward for episode 34 is 33.0\n",
      "Total Reward for episode 35 is 70.0\n",
      "Total Reward for episode 36 is 95.0\n",
      "Total Reward for episode 37 is -300.0\n",
      "Total Reward for episode 38 is 95.0\n",
      "Total Reward for episode 39 is -300.0\n",
      "Total Reward for episode 40 is 95.0\n",
      "Total Reward for episode 41 is 95.0\n",
      "Total Reward for episode 42 is 62.0\n",
      "Total Reward for episode 43 is 95.0\n",
      "Total Reward for episode 44 is 95.0\n",
      "Total Reward for episode 45 is 95.0\n",
      "Total Reward for episode 46 is 66.0\n",
      "Total Reward for episode 47 is 95.0\n",
      "Total Reward for episode 48 is -300.0\n",
      "Total Reward for episode 49 is 66.0\n",
      "Total Reward for episode 50 is 95.0\n",
      "Total Reward for episode 51 is 95.0\n",
      "Total Reward for episode 52 is 95.0\n",
      "Total Reward for episode 53 is 56.0\n",
      "Total Reward for episode 54 is 95.0\n",
      "Total Reward for episode 55 is 95.0\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-c2c70f4c4bb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-147-b393eb591340>\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m#resets game\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen_buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrayscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "for episode in range(100): \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        #change to slow down frame\n",
    "        #time.sleep(0.20)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(episode, total_reward))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cb5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
